---
title: "DAM: Hierarchical Adaptive Feature Selection Using Convolution Encoder Decoder Network for Strawberry Segmentation"
collection: publications
permalink: /publication/2021-02-DAM
excerpt: "Autonomous harvesters and Strawberry Segmentation"
date: 2021-09-11
venue: 'Frontiers in Plant Science'
imageurl: '/images/publications/DAM.PNG'
paperurl: '/files/dam.pdf'
link: 'https://doi.org/10.3389/fpls.2021.591333'
citation: 'Ilyas, T.;. <strong>Khan. A</strong>,  ; Umraiz, M.; H. Kim,, (2020). &quot;DAM: Hierarchical Adaptive Feature Selection
Using Convolution Encoder Decoder Network for Strawberry Segmentation  .&quot; <i>Physical Review NOT Letters, 121</i>(11). doi:10.3389/fpls.2021.591333'
---
<center><img src = '/images/publications/DAM.png'></center>
## Abstract
Autonomous harvesters can be used for the timely cultivation of high-value crops
such as strawberries, where the robots have the capability to identify ripe and unripe
crops. However, the real-time segmentation of strawberries in an unbridled farming
environment is a challenging task due to fruit occlusion by multiple trusses, stems, and
leaves. In this work, we propose a possible solution by constructing a dynamic feature
selection mechanism for convolutional neural networks (CNN). The proposed building
block namely a dense attention module (DAM) controls the flow of information between
the convolutional encoder and decoder. DAM enables hierarchical adaptive feature
fusion by exploiting both inter-channel and intra-channel relationships and can be easily
integrated into any existing CNN to obtain category-specific feature maps. We validate
our attention module through extensive ablation experiments. In addition, a dataset is
collected from different strawberry farms and divided into four classes corresponding to
different maturity levels of fruits and one is devoted to background. Quantitative analysis
of the proposed method showed a 4.1% and 2.32% increase in mean intersection over
union, over existing state-of-the-art semantic segmentation models and other attention
modules respectively, while simultaneously retaining a processing speed of 53 frames
per second.

## Files
- [Paper](/files/dam.pdf)
